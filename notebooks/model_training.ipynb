{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb79e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import clone\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Processed Data\n",
    "data_path = '../data/processed/'\n",
    "\n",
    "\n",
    "# Load Features\n",
    "X_train = pd.read_parquet(f'{data_path}X_train_processed.parquet')\n",
    "X_test = pd.read_parquet(f'{data_path}X_test_processed.parquet')\n",
    "\n",
    "# Load Targets (Series)\n",
    "y_train_log = pd.read_parquet(f'{data_path}y_train_log.parquet')['Price']\n",
    "y_test_log = pd.read_parquet(f'{data_path}y_test_log.parquet')['Price']\n",
    "\n",
    "#Create Real Dollar Targets for Evaluation\n",
    "#Reverse the log transformation to measure error in real money ($)\n",
    "y_test_real = np.expm1(y_test_log)\n",
    "\n",
    "print(\"Data Loaded Successfully.\")\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Test Data Shape:     {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Models\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=1000,       \n",
    "        learning_rate=0.05,      #Slower learning for better accuracy\n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        n_estimators=1000, \n",
    "        learning_rate=0.05, \n",
    "        n_jobs=-1, \n",
    "        random_state=42, \n",
    "        verbose=-1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995573ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Cross-Validation\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Starting Model Selection with Cross-Validation...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"   Evaluating {name}...\")\n",
    "    \n",
    "    #Cross Validation (Strictly on Training Data)\n",
    "    #Use Negative MAE because sklearn metrics are \"higher is better\"\n",
    "    #Evaluating on Log Prices, so this is \"Log MAE\"\n",
    "    #cv=5 means split X_train into 5 parts, train on 4, test on 1, repeat 5 times.\n",
    "    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    #Convert to positive\n",
    "    avg_log_mae = -1 * cv_scores.mean()\n",
    "    \n",
    "    #Fit on full training data (For final testing later)\n",
    "    #Need to fit the model to use it for Feature Importance / Final Test\n",
    "    model.fit(X_train, y_train_log)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    #Save Metrics\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV MAE (Log)\": avg_log_mae\n",
    "    })\n",
    "\n",
    "print(\"\\nCross-Validation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Results\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"CV MAE (Log)\", ascending=True)\n",
    "\n",
    "print(\"\\nMODEL SELECTION LEADERBOARD (Based on CV)\")\n",
    "\n",
    "#Style the dataframe for easy reading\n",
    "#Greens_r because lower error is better\n",
    "display(results_df.style.background_gradient(cmap=\"Greens_r\", subset=[\"CV MAE (Log)\"])) \n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "#Plot MAE (Lower is better)\n",
    "sns.barplot(data=results_df, x=\"Model\", y=\"CV MAE (Log)\", palette=\"viridis\")\n",
    "plt.title(\"Model Comparison: Cross-Validation Error (Log Scale)\")\n",
    "plt.ylabel(\"Mean Absolute Error (Log Units)\")\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Evaluation of the Winner\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"The Winner (based on CV) is: {best_model_name}\")\n",
    "\n",
    "#Evaluate on Test Set\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred_final = np.expm1(y_pred_log)\n",
    "\n",
    "#Calculate Real Dollar Metrics\n",
    "final_mae = mean_absolute_error(y_test_real, y_pred_final)\n",
    "final_r2 = r2_score(y_test_real, y_pred_final)\n",
    "\n",
    "print(f\"Final Test Set MAE: ${final_mae:,.0f}\")\n",
    "print(f\"Final Test Set R2:  {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning for the Best Model\n",
    "print(f\"Starting Hyperparameter Tuning for: {best_model_name}\")\n",
    "\n",
    "#Define Parameter Grids for each model family\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [500, 1000, 2000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        'n_estimators': [500, 1000, 2000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'max_depth': [-1, 10, 20],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "#Select the Grid\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    base_model = clone(models[best_model_name]) #Get the original model instance\n",
    "    \n",
    "    #Setup Randomized Search\n",
    "    #n_iter=20 means try 20 random combinations\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        scoring='neg_mean_absolute_error', #Optimize for MAE\n",
    "        cv=3, #3-Fold Cross Validation\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    #Run Search\n",
    "    print(\"Running RandomizedSearchCV\")\n",
    "    random_search.fit(X_train, y_train_log)\n",
    "    \n",
    "    #Get Best Results\n",
    "    tuned_model = random_search.best_estimator_\n",
    "    print(f\"\\nTuning Complete. Best Params: {random_search.best_params_}\")\n",
    "    \n",
    "    #Evaluate Tuned Model\n",
    "    y_pred_log_tuned = tuned_model.predict(X_test)\n",
    "    y_pred_tuned = np.expm1(y_pred_log_tuned)\n",
    "    \n",
    "    mae_tuned = mean_absolute_error(y_test_real, y_pred_tuned)\n",
    "    \n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    print(f\"Original {best_model_name} MAE: ${final_mae:,.0f}\")\n",
    "    print(f\"Tuned {best_model_name} MAE:    ${mae_tuned:,.0f}\")\n",
    "    \n",
    "    if mae_tuned < final_mae:\n",
    "        print(\"Improvement! The tuned model is better.\")\n",
    "    else:\n",
    "        print(\"No improvement. The default parameters were already quite good.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping tuning for {best_model_name} (No grid defined or Linear Regression).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Best Model\n",
    "\n",
    "print(\"Saving the best model...\")\n",
    "\n",
    "#Create models directory\n",
    "model_dir = '../models/'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "#Determine which model was actually better (Tuned vs Original)\n",
    "\n",
    "#Compare the Tuned Model's Test MAE vs the Original Model's Test MAE\n",
    "#Check if 'tuned_model' exists and if it improved the score\n",
    "if 'tuned_model' in locals() and 'mae_tuned' in locals() and mae_tuned < final_mae:\n",
    "    final_model = tuned_model\n",
    "    final_name = f\"{best_model_name}_tuned\"\n",
    "    print(f\"Selecting Tuned {best_model_name} (MAE: ${mae_tuned:,.0f})\")\n",
    "else:\n",
    "    final_model = best_model\n",
    "    final_name = best_model_name\n",
    "    print(f\"Selecting Original {best_model_name} (MAE: ${final_mae:,.0f})\")\n",
    "\n",
    "#Save\n",
    "save_path = f'{model_dir}house_price_model.joblib'\n",
    "joblib.dump(final_model, save_path)\n",
    "\n",
    "print(f\"Model saved successfully to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
