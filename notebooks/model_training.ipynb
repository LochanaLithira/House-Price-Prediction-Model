{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb79e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Model Libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore') # Clean up clutter in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD THE DATA ---\n",
    "data_path = '../data/processed/'\n",
    "\n",
    "print(\"‚è≥ Loading data from Parquet files...\")\n",
    "\n",
    "# Load Features\n",
    "X_train = pd.read_parquet(f'{data_path}X_train_processed.parquet')\n",
    "X_test = pd.read_parquet(f'{data_path}X_test_processed.parquet')\n",
    "\n",
    "# Load Targets (Series)\n",
    "y_train_log = pd.read_parquet(f'{data_path}y_train_log.parquet')['Price']\n",
    "y_test_log = pd.read_parquet(f'{data_path}y_test_log.parquet')['Price']\n",
    "\n",
    "# Create Real Dollar Targets for Evaluation\n",
    "# We must reverse the log transformation to measure error in real money ($)\n",
    "y_test_real = np.expm1(y_test_log)\n",
    "\n",
    "print(\"‚úÖ Data Loaded Successfully.\")\n",
    "print(f\"   Training Data Shape: {X_train.shape}\")\n",
    "print(f\"   Test Data Shape:     {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINE MODEL DICTIONARY ---\n",
    "# We compare different \"families\" of algorithms to find the best fit.\n",
    "\n",
    "models = {\n",
    "    # 1. Baseline: Simple linear relationships\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \n",
    "    # 2. Bagging: Robust to outliers, handles non-linear data well\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    # 3. Boosting (XGB): Optimized gradient boosting, industry standard\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=1000,       # More trees\n",
    "        learning_rate=0.05,      # Slower learning for better accuracy\n",
    "        n_jobs=-1, \n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # 4. Boosting (LGBM): Very fast, handles large datasets well\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        n_estimators=1000, \n",
    "        learning_rate=0.05, \n",
    "        n_jobs=-1, \n",
    "        random_state=42, \n",
    "        verbose=-1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995573ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# --- TRAINING LOOP (CROSS-VALIDATION) ---\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(\"üöÄ Starting Model Selection with Cross-Validation...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"   Evaluating {name}...\")\n",
    "    \n",
    "    # A. CROSS-VALIDATION (Strictly on Training Data)\n",
    "    # We use Negative MAE because sklearn metrics are \"higher is better\" (so they return negative error)\n",
    "    # We are evaluating on Log Prices, so this is \"Log MAE\"\n",
    "    # cv=5 means we split X_train into 5 parts, train on 4, test on 1, repeat 5 times.\n",
    "    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    \n",
    "    # Convert to positive\n",
    "    avg_log_mae = -1 * cv_scores.mean()\n",
    "    \n",
    "    # B. FIT ON FULL TRAINING DATA (For final testing later)\n",
    "    # We still need to fit the model to use it for Feature Importance / Final Test\n",
    "    model.fit(X_train, y_train_log)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # C. SAVE METRICS\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV MAE (Log)\": avg_log_mae\n",
    "    })\n",
    "\n",
    "print(\"\\n‚úÖ Cross-Validation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DISPLAY RESULTS ---\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"CV MAE (Log)\", ascending=True)\n",
    "\n",
    "print(\"\\nüèÜ MODEL SELECTION LEADERBOARD (Based on CV) üèÜ\")\n",
    "# Style the dataframe for easy reading\n",
    "# Greens_r because lower error is better\n",
    "display(results_df.style.background_gradient(cmap=\"Greens_r\", subset=[\"CV MAE (Log)\"])) \n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot MAE (Lower is better)\n",
    "sns.barplot(data=results_df, x=\"Model\", y=\"CV MAE (Log)\", palette=\"viridis\")\n",
    "plt.title(\"Model Comparison: Cross-Validation Error (Log Scale)\")\n",
    "plt.ylabel(\"Mean Absolute Error (Log Units)\")\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL EVALUATION OF THE WINNER ---\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"üåü The Winner (based on CV) is: {best_model_name}\")\n",
    "\n",
    "# Now we open the \"Test Vault\" ONLY for the winner\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred_final = np.expm1(y_pred_log)\n",
    "\n",
    "# Calculate Real Dollar Metrics\n",
    "final_mae = mean_absolute_error(y_test_real, y_pred_final)\n",
    "final_r2 = r2_score(y_test_real, y_pred_final)\n",
    "\n",
    "print(f\"üí∞ Final Test Set MAE: ${final_mae:,.0f}\")\n",
    "print(f\"üìä Final Test Set R2:  {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- HYPERPARAMETER TUNING ---\n",
    "print(f\"üîß Starting Hyperparameter Tuning for: {best_model_name}\")\n",
    "\n",
    "# 1. Define Parameter Grids for each model family\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [500, 1000, 2000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        'n_estimators': [500, 1000, 2000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'max_depth': [-1, 10, 20],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Select the Grid\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    base_model = clone(models[best_model_name]) # Get the original model instance\n",
    "    \n",
    "    # 3. Setup Randomized Search\n",
    "    # n_iter=20 means we try 20 random combinations (adjust for speed vs accuracy)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        scoring='neg_mean_absolute_error', # Optimize for MAE\n",
    "        cv=3, # 3-Fold Cross Validation\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 4. Run Search\n",
    "    print(\"   Running RandomizedSearchCV (this may take a while)...\")\n",
    "    random_search.fit(X_train, y_train_log)\n",
    "    \n",
    "    # 5. Get Best Results\n",
    "    tuned_model = random_search.best_estimator_\n",
    "    print(f\"\\n‚úÖ Tuning Complete. Best Params: {random_search.best_params_}\")\n",
    "    \n",
    "    # 6. Evaluate Tuned Model\n",
    "    y_pred_log_tuned = tuned_model.predict(X_test)\n",
    "    y_pred_tuned = np.expm1(y_pred_log_tuned)\n",
    "    \n",
    "    mae_tuned = mean_absolute_error(y_test_real, y_pred_tuned)\n",
    "    \n",
    "    print(f\"\\nüìä Performance Comparison:\")\n",
    "    print(f\"   Original {best_model_name} MAE: ${final_mae:,.0f}\")\n",
    "    print(f\"   Tuned {best_model_name} MAE:    ${mae_tuned:,.0f}\")\n",
    "    \n",
    "    if mae_tuned < final_mae:\n",
    "        print(\"üéâ Improvement! The tuned model is better.\")\n",
    "    else:\n",
    "        print(\"ü§∑ No improvement. The default parameters were already quite good.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping tuning for {best_model_name} (No grid defined or Linear Regression).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- SAVE THE BEST MODEL ---\n",
    "print(\"üíæ Saving the best model...\")\n",
    "\n",
    "# Create models directory\n",
    "model_dir = '../models/'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Determine which model was actually better (Tuned vs Original)\n",
    "# We compare the Tuned Model's Test MAE vs the Original Model's Test MAE\n",
    "# We check if 'tuned_model' exists and if it improved the score\n",
    "if 'tuned_model' in locals() and 'mae_tuned' in locals() and mae_tuned < final_mae:\n",
    "    final_model = tuned_model\n",
    "    final_name = f\"{best_model_name}_tuned\"\n",
    "    print(f\"   Selecting Tuned {best_model_name} (MAE: ${mae_tuned:,.0f})\")\n",
    "else:\n",
    "    final_model = best_model\n",
    "    final_name = best_model_name\n",
    "    print(f\"   Selecting Original {best_model_name} (MAE: ${final_mae:,.0f})\")\n",
    "\n",
    "# Save\n",
    "save_path = f'{model_dir}house_price_model.joblib'\n",
    "joblib.dump(final_model, save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12ca43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
