{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for cleaner output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the raw dataset\n",
    "df=pd.read_csv('../data/raw/Melbourne_housing_FULL.csv')\n",
    "#Basic check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798896f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data types\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the 'Date' feature to datetime object\n",
    "df['Date']=pd.to_datetime(df['Date'], dayfirst=True)\n",
    "\n",
    "#Verification\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate check \n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
    "#Show the duplicated rows\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicated rows\n",
    "df=df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "#Verification\n",
    "print(\"Number of duplicate rows after cleaning :\",df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check \n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many rows (buildings) that have Building Area less than 5 or greater than 10000\n",
    "print(\"Number of Buildings that have Building Area less than 5:\", len(df[df['BuildingArea'] < 5]))\n",
    "print(\"Number of Buildings that have Building Area more than 10000:\", len(df[df['BuildingArea'] > 10000]))\n",
    "\n",
    "#Replace those impossible values with NaN \n",
    "df.loc[df['BuildingArea'] < 5, 'BuildingArea'] = np.nan\n",
    "df.loc[df['BuildingArea'] > 10000, 'BuildingArea'] = np.nan\n",
    "\n",
    "#Verification\n",
    "print(\"Number of Buildings that have Building Area less than 5 after cleaning:\", len(df[df['BuildingArea'] < 5]))\n",
    "print(\"Number of Buildings that have Building Area more than 10000 after cleaning:\", len(df[df['BuildingArea'] > 10000]))\n",
    "\n",
    "#Number of missing values in the feature\n",
    "print(\"Number of missing values :\",df['BuildingArea'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42778fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find impossible YearBuilt values\n",
    "\n",
    "#Get the dynamic current year\n",
    "current_year=datetime.datetime.now().year\n",
    "\n",
    "#Find count of properties that build before 1800 and after ccurrent year\n",
    "print(\"Number of properties with impossible YearBuilt values:\", ((df['YearBuilt'] < 1800) | (df['YearBuilt'] >current_year)).sum())\n",
    "\n",
    "#Fix the impossible YearBuilt values by replacing them with NaN\n",
    "df['YearBuilt'] = df['YearBuilt'].where(df['YearBuilt'].between(1800, current_year), np.nan)\n",
    "\n",
    "#Verification\n",
    "print(\"Number of properties with impossible YearBuilt values after cleaning:\", ((df['YearBuilt'] < 1800) | (df['YearBuilt'] > current_year)).sum())\n",
    "\n",
    "#Number of missing value in the feature\n",
    "print(\"Number of missing values :\", df['YearBuilt'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ca160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check impossible properties with 0 bathrooms\n",
    "print(\"Count of properties with 0 bathrooms:\", (df['Bathroom'] == 0).sum())\n",
    "\n",
    "#Logic of replacing 0 bathrooms with NaN\n",
    "\n",
    "#If the bathroom is 0 and the property type is house and the year built is after 1900, then it is likely an error\n",
    "df.loc[(df['Type'] == 'h') & (df['YearBuilt'] > 1900) & (df['Bathroom'] == 0), 'Bathroom'] = np.nan\n",
    "\n",
    "#If the bathroom is 0 and the property type is unit or townhouse, then it is likely an error\n",
    "df.loc[(df['Type'].isin(['u', 't'])) & (df['Bathroom'] == 0), 'Bathroom'] = np.nan\n",
    "\n",
    "#Verification\n",
    "print(\"Count of properties with 0 bathrooms after cleaning:\", (df['Bathroom'] == 0).sum())\n",
    "\n",
    "#Number of missing values in the feature\n",
    "print(\"Number of missing values :\", df['Bathroom'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inconsistency Scan\n",
    "text_features = df.select_dtypes(include=['object']).columns\n",
    "for col in text_features:\n",
    "    unique_count=df[col].nunique()\n",
    "    print(f\"Feature [{col}] has {unique_count} unique values.\")\n",
    "    print(sorted(df[col].unique().astype(str)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff86f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the inconsistencies\n",
    "\n",
    "# Convert the 'Suburb' feature to title case\n",
    "df['Suburb'] = df['Suburb'].str.title()\n",
    "\n",
    "#Convert 'SellerG' to remove branch info and fix casing\n",
    "df['SellerG'] = df['SellerG'].str.split('/').str[0].str.strip().str.title()\n",
    "\n",
    "#Verfication\n",
    "print(\"--- Suburb ---\")\n",
    "print(sorted(df[df['Suburb'].str.contains('Croydon', case=False)]['Suburb'].unique()))\n",
    "print(sorted(df[df['Suburb'].str.contains('Viewbank', case=False)]['Suburb'].unique()))\n",
    "\n",
    "print(\"\\n--- SellerG ---\")\n",
    "brands_to_check = ['Buxton', 'Hockingstuart', 'Vicprop']\n",
    "for brand in brands_to_check:\n",
    "    print(f\"{brand} variations: {sorted([s for s in df['SellerG'].unique() if brand.lower() in s.lower()])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ceb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate number of missing values per feature\n",
    "print(\"Number of missing values per feature: \\n\", df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the missing values of target feature \n",
    "df=df.dropna(subset=['Price'])\n",
    "\n",
    "#Drop the missing values of the Postcode feature\n",
    "df=df.dropna(subset=['Postcode'])\n",
    "\n",
    "#Verification \n",
    "print(\"Number of missing values per feature after dropping target and Postcode missing values:\\n\", df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Data Splitting Strategy ---\n",
    "# 1. Define Features (X) and Target (y)\n",
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "# 2. Perform Random Split (User Decision: Random Split for Generalization)\n",
    "# shuffle=True ensures we mix 2016, 2017, and 2018 data together\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Verify\n",
    "print(\"--- Random Split Successful ---\")\n",
    "print(f\"Training Set: {X_train.shape[0]} rows\")\n",
    "print(f\"Test Set:     {X_test.shape[0]} rows\")\n",
    "\n",
    "# 4. SAFETY CHECK\n",
    "assert len(X_train) + len(X_test) == len(df), \"Error: Row mismatch!\"\n",
    "print(\"Safety Check Passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "#Extract the Sold Year from the 'Date' feature\n",
    "X_train['SoldYear'] = pd.to_datetime(X_train['Date']).dt.year\n",
    "X_test['SoldYear'] = pd.to_datetime(X_test['Date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Features\n",
    "\n",
    "features_to_drop = ['Date', 'Address', 'Bedroom2', 'CouncilArea', 'Method', 'Postcode']\n",
    "\n",
    "X_train = X_train.drop(columns=features_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=features_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rare Category Grouping\n",
    "\n",
    "#Group all specific rural regions into one \"Regional Victoria\" category.\n",
    "regions_to_group = ['Eastern Victoria', 'Northern Victoria', 'Western Victoria']\n",
    "\n",
    "#Apply to training and test sets\n",
    "X_train['Regionname'] = X_train['Regionname'].replace(regions_to_group, 'Regional Victoria')\n",
    "X_test['Regionname'] = X_test['Regionname'].replace(regions_to_group, 'Regional Victoria')\n",
    "\n",
    "#Verification\n",
    "print(X_train['Regionname'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d68815",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handeling Missing values (Imputation)\n",
    "\n",
    "#Simple Imputations\n",
    "# A. Regionname (Categorical) -> Use MODE (Most Frequent)\n",
    "# We use [0] to get the first value if there's a tie\n",
    "mode_region = X_train['Regionname'].mode()[0]\n",
    "X_train['Regionname'] = X_train['Regionname'].fillna(mode_region)\n",
    "X_test['Regionname'] = X_test['Regionname'].fillna(mode_region)\n",
    "\n",
    "# B. Propertycount (Numerical) -> Use MEDIAN\n",
    "median_prop = X_train['Propertycount'].median()\n",
    "X_train['Propertycount'] = X_train['Propertycount'].fillna(median_prop)\n",
    "X_test['Propertycount'] = X_test['Propertycount'].fillna(median_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Imputation for 'Lattitude' and 'Longtitude' using Suburb Centroids\n",
    "\n",
    "# --- STEP 1: Compute suburb centroids from training data ---\n",
    "suburb_coords = X_train.groupby('Suburb')[['Lattitude', 'Longtitude']].mean()\n",
    "\n",
    "# Count how many houses each suburb has\n",
    "suburb_counts = X_train['Suburb'].value_counts()\n",
    "\n",
    "# Minimum number of houses to consider a suburb \"reliable\"\n",
    "min_houses = 3\n",
    "reliable_suburbs = suburb_counts[suburb_counts >= min_houses].index\n",
    "\n",
    "# Robust centroids for reliable suburbs\n",
    "robust_centroids = suburb_coords.loc[reliable_suburbs]\n",
    "\n",
    "# Global median fallback\n",
    "global_lat = X_train['Lattitude'].median()\n",
    "global_long = X_train['Longtitude'].median()\n",
    "\n",
    "# --- STEP 2: Map centroids for TRAINING SET ---\n",
    "\n",
    "# Map reliable centroids, leave others as NaN\n",
    "train_lat = X_train['Suburb'].map(robust_centroids['Lattitude'])\n",
    "train_long = X_train['Suburb'].map(robust_centroids['Longtitude'])\n",
    "\n",
    "# Fill only missing values with mapped centroid\n",
    "X_train['Lattitude'] = X_train['Lattitude'].fillna(train_lat)\n",
    "X_train['Longtitude'] = X_train['Longtitude'].fillna(train_long)\n",
    "\n",
    "# Fill any remaining NaNs with global median\n",
    "X_train['Lattitude'] = X_train['Lattitude'].fillna(global_lat)\n",
    "X_train['Longtitude'] = X_train['Longtitude'].fillna(global_long)\n",
    "\n",
    "# --- STEP 3: Map centroids for TEST SET ---\n",
    "\n",
    "# Map centroids from training data\n",
    "test_lat = X_test['Suburb'].map(robust_centroids['Lattitude'])\n",
    "test_long = X_test['Suburb'].map(robust_centroids['Longtitude'])\n",
    "\n",
    "# Fill missing values with mapped centroid\n",
    "X_test['Lattitude'] = X_test['Lattitude'].fillna(test_lat)\n",
    "X_test['Longtitude'] = X_test['Longtitude'].fillna(test_long)\n",
    "\n",
    "# Fill any remaining NaNs with global median\n",
    "X_test['Lattitude'] = X_test['Lattitude'].fillna(global_lat)\n",
    "X_test['Longtitude'] = X_test['Longtitude'].fillna(global_long)\n",
    "\n",
    "print(\"‚úÖ Vectorized Advanced Suburb Centroid Imputation Complete for Train & Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d976d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- STEP 1: Encode Categoricals (Strings -> Integers) ---\n",
    "# We keep the encoder to reverse this later if needed\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns\n",
    "ord_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train[cat_cols] = ord_encoder.fit_transform(X_train[cat_cols])\n",
    "X_test[cat_cols] = ord_encoder.transform(X_test[cat_cols])\n",
    "\n",
    "# --- STEP 2: SCALING (Crucial for KNN) ---\n",
    "# Squash everything to 0-1 so 'Landsize' doesn't overpower 'Car'\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale and keep as DataFrame (to track columns)\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# --- STEP 3: KNN Imputation ---\n",
    "# KNN finds neighbors using the scaled data\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "X_train_imputed = pd.DataFrame(knn_imputer.fit_transform(X_train_scaled), columns=X_train.columns, index=X_train.index)\n",
    "X_test_imputed = pd.DataFrame(knn_imputer.transform(X_test_scaled), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# --- STEP 4: INVERSE TRANSFORM & CLEANUP ---\n",
    "# 1. Bring everything back to original scale (Year=2017, Region=5.0)\n",
    "X_train_final = pd.DataFrame(scaler.inverse_transform(X_train_imputed), columns=X_train.columns, index=X_train.index)\n",
    "X_test_final = pd.DataFrame(scaler.inverse_transform(X_test_imputed), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# 2. ROUND Categoricals (The Critical Fix)\n",
    "# Inverse transform might give 4.9999 instead of 5. We must round to nearest integer.\n",
    "# We have to combine categorical columns + Numerical columns for this step (Didnt include Distance,BuildingArea etc. as they can be float)\n",
    "\n",
    "cols_to_round = list(cat_cols) + ['Bathroom', 'Car', 'YearBuilt', 'Rooms', 'Propertycount', 'SoldYear']\n",
    "\n",
    "for col in cols_to_round:\n",
    "    if col in X_train_final.columns:\n",
    "        # Round to nearest whole number and cast to Integer\n",
    "        X_train_final[col] = X_train_final[col].round(0).astype(int)\n",
    "        X_test_final[col] = X_test_final[col].round(0).astype(int)\n",
    "\n",
    "\n",
    "# 3. Update main variables\n",
    "X_train = X_train_final\n",
    "X_test = X_test_final\n",
    "\n",
    "print(\"‚úÖ Imputation Complete.\")\n",
    "print(f\"Missing Values: \\n{X_train.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd40274",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAFETY CHECK: Verify no negatives before Log Transform ---\n",
    "skewed_features = ['Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Propertycount']\n",
    "\n",
    "# Check for negatives in Train and Test\n",
    "negatives_train = (X_train[skewed_features] < 0).sum().sum()\n",
    "negatives_test = (X_test[skewed_features] < 0).sum().sum()\n",
    "\n",
    "if negatives_train == 0 and negatives_test == 0:\n",
    "    print(\"‚úÖ Safe to Proceed: All features are non-negative.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è STOP! Found {negatives_train} negatives in Train and {negatives_test} in Test.\")\n",
    "    print(\"   Do NOT apply Log Transform to negative values. Check if you scaled too early.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming Skewed Features with LogTransform\n",
    "\n",
    "#Target Transformation\n",
    "# We transform y_train so the model learns a Normal Distribution\n",
    "# We transform y_test ONLY for checking error metrics later (Optional but good practice)\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "#Feature Transformation\n",
    "skewed_features = ['Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Propertycount']\n",
    "\n",
    "#Apply np.log1p to these features in both Train and Test\n",
    "for col in skewed_features:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = np.log1p(X_train[col])\n",
    "        X_test[col] = np.log1p(X_test[col])\n",
    "        print(\"Log transformed :\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Features\n",
    "\n",
    "# --- DEFINE GROUPS ---\n",
    "robust_features = ['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Propertycount']\n",
    "standard_features = ['YearBuilt', 'Lattitude', 'Longtitude']\n",
    "\n",
    "# --- STEP: CHECK FOR ZERO VARIANCE / ZERO IQR ---\n",
    "# We need to know if RobustScaler or StandardScaler will fail silently.\n",
    "\n",
    "# 1. Check Standard Deviation (for StandardScaler features)\n",
    "std_check = X_train[standard_features].std()\n",
    "constant_std = std_check[std_check == 0].index.tolist()\n",
    "\n",
    "if constant_std:\n",
    "    print(f\"‚ö†Ô∏è WARNING: The following columns have ZERO variance (Constant): {constant_std}\")\n",
    "    print(\"   Action: Drop these columns before scaling, or the model will ignore them.\")\n",
    "else:\n",
    "    print(\"‚úÖ StandardScaler Safety Check Passed (No constant columns).\")\n",
    "\n",
    "# 2. Check IQR (for RobustScaler features)\n",
    "# Calculate Q1 (25%) and Q3 (75%)\n",
    "Q1 = X_train[robust_features].quantile(0.25)\n",
    "Q3 = X_train[robust_features].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Find columns where IQR is 0\n",
    "zero_iqr = IQR[IQR == 0].index.tolist()\n",
    "\n",
    "if zero_iqr:\n",
    "    print(f\"‚ö†Ô∏è WARNING: The following columns have an IQR of 0: {zero_iqr}\")\n",
    "else:\n",
    "    print(\"‚úÖ RobustScaler Safety Check Passed (No zero-IQR columns).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3def4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROBUST SCALER\n",
    "scaler_rob = RobustScaler()\n",
    "\n",
    "# We use .loc to ensure we update the specific columns in place\n",
    "# Check if columns exist to prevent errors\n",
    "existing_robust = [col for col in robust_features if col in X_train.columns]\n",
    "\n",
    "if existing_robust:\n",
    "    X_train[existing_robust] = scaler_rob.fit_transform(X_train[existing_robust])\n",
    "    X_test[existing_robust] = scaler_rob.transform(X_test[existing_robust])\n",
    "    print(f\"‚úÖ Applied RobustScaler to: {existing_robust}\")\n",
    "\n",
    "#STANDARD SCALER\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "existing_std = [col for col in standard_features if col in X_train.columns]\n",
    "\n",
    "if existing_std:\n",
    "    X_train[existing_std] = scaler_std.fit_transform(X_train[existing_std])\n",
    "    X_test[existing_std] = scaler_std.transform(X_test[existing_std])\n",
    "    print(f\"‚úÖ Applied StandardScaler to: {existing_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e769ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- PART 1: ONE-HOT ENCODING (Low Cardinality) ---\n",
    "# Features: 'Type', 'Regionname'\n",
    "# Logic: Best for categories with few options (<10).\n",
    "# Industrial Standard: Use OneHotEncoder(handle_unknown='ignore') to safely handle new categories in Test.\n",
    "\n",
    "ohe_cols = ['Type', 'Regionname']\n",
    "\n",
    "# 1. Setup Encoder\n",
    "# sparse_output=False creates a regular pandas DataFrame, not a compressed matrix.\n",
    "# dtype=int makes the output 0/1 instead of 0.0/1.0.\n",
    "encoder_ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=int).set_output(transform='pandas')\n",
    "\n",
    "# 2. Fit & Transform\n",
    "# We fit only on Train to learn the categories.\n",
    "X_train_ohe = encoder_ohe.fit_transform(X_train[ohe_cols])\n",
    "X_test_ohe = encoder_ohe.transform(X_test[ohe_cols])\n",
    "\n",
    "# 3. Merge & Clean\n",
    "# Attach new columns and drop the old text columns\n",
    "X_train = pd.concat([X_train, X_train_ohe], axis=1).drop(columns=ohe_cols)\n",
    "X_test = pd.concat([X_test, X_test_ohe], axis=1).drop(columns=ohe_cols)\n",
    "\n",
    "print(f\"‚úÖ One-Hot Encoding Complete. Added {X_train_ohe.shape[1]} binary columns.\")\n",
    "\n",
    "\n",
    "# --- PART 2: TARGET ENCODING (High Cardinality) ---\n",
    "# Features: 'Suburb', 'SellerG'\n",
    "# Logic: Best for categories with hundreds of options.\n",
    "# Industrial Standard: Use 'category_encoders' with smoothing to prevent overfitting on rare suburbs.\n",
    "\n",
    "target_cols = ['Suburb', 'SellerG']\n",
    "\n",
    "# 1. Setup Encoder\n",
    "# smoothing=10.0: Higher values mean we trust the \"Global Average\" more for rare categories.\n",
    "# This is the \"Safety Net\" that manual coding often misses.\n",
    "encoder_target = TargetEncoder(cols=target_cols, smoothing=10.0)\n",
    "\n",
    "# 2. Fit & Transform\n",
    "# CRITICAL: We fit on X_train and y_train_log.\n",
    "# The model learns the relationship between Suburb -> Log Price.\n",
    "X_train[target_cols] = encoder_target.fit_transform(X_train[target_cols], y_train_log)\n",
    "\n",
    "# 3. Transform Test\n",
    "# We use the learned patterns to transform Test (no peeking at y_test!)\n",
    "X_test[target_cols] = encoder_target.transform(X_test[target_cols])\n",
    "\n",
    "print(f\"‚úÖ Target Encoding (Industrial) Complete on: {target_cols}\")\n",
    "\n",
    "\n",
    "# --- PART 3: FINAL SAFETY CHECK (Crucial) ---\n",
    "# Industrial Standard: Ensure Test set has exact same columns in exact same order.\n",
    "# This prevents \"Silent Failures\" where columns get swapped.\n",
    "\n",
    "# 1. Reindex Test to match Train\n",
    "# Drops any extra columns in Test, adds missing ones (filling with 0), and fixes order.\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# 2. Strict Assertion\n",
    "# If this line fails, the pipeline stops immediately.\n",
    "assert list(X_train.columns) == list(X_test.columns), \"‚ùå CRITICAL ERROR: Column mismatch between Train and Test!\"\n",
    "\n",
    "print(\"\\nüöÄ Encoding & Alignment Successful.\")\n",
    "print(f\"Final Data Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- FINAL STEP: SAVE PROCESSED DATA ---\n",
    "# We save in 'parquet' format because it preserves your integer columns (Car, YearBuilt).\n",
    "# CSV is bad because it might turn them back into text or floats.\n",
    "\n",
    "save_path = '../data/processed/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving Processed Data...\")\n",
    "\n",
    "# Save Features\n",
    "X_train.to_parquet(f'{save_path}X_train_processed.parquet')\n",
    "X_test.to_parquet(f'{save_path}X_test_processed.parquet')\n",
    "\n",
    "# Save Targets\n",
    "# We convert to DataFrame because Series cannot be saved as Parquet directly\n",
    "pd.DataFrame(y_train_log, columns=['Price']).to_parquet(f'{save_path}y_train_log.parquet')\n",
    "pd.DataFrame(y_test_log, columns=['Price']).to_parquet(f'{save_path}y_test_log.parquet')\n",
    "\n",
    "# Optional: Save the \"Real Dollar\" y_test for final error checking later\n",
    "pd.DataFrame(y_test, columns=['Price']).to_parquet(f'{save_path}y_test_real.parquet')\n",
    "\n",
    "print(f\"‚úÖ success! Processed data saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
