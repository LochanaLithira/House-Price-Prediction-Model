{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import scipy.stats as ss\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dde6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Settings\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', None)  #This displays all columns in the dataframe\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)  # Display describe() with normal float formatting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff194d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df=pd.read_csv(\"../data/raw/Melbourne_housing_FULL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207228b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b414af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structure of the dataset\n",
    "rows=df.shape[0]\n",
    "columns=df.shape[1]\n",
    "print(\"Number of rows:\", rows )\n",
    "print(\"Number of columns:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Types & Missing Counts\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "\n",
    "# Verification\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate check(rows)\n",
    "duplicates=df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicates)\n",
    "\n",
    "# Show duplicated rows\n",
    "df[df.duplicated()]\n",
    "\n",
    "#Remove duplicates\n",
    "df=df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "#Verify\n",
    "print(f\"Duplicates after: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad789ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many impossible BuildingArea values are there (<5.0)\n",
    "print(f\"Count of Impossible BuildingArea values: {len(df[df['BuildingArea'] < 5])}\")\n",
    "\n",
    "# The Fix: Replace values < 5.0 with NaN (Missing) \n",
    "df.loc[df['BuildingArea'] < 5, 'BuildingArea'] = np.nan\n",
    "\n",
    "print(f\"Count of Impossible BuildingArea values after fix:{len(df[df['BuildingArea'] < 5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d87518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dynamic current year\n",
    "current_year=datetime.datetime.now().year\n",
    "\n",
    "#Check count of invalid years before fix\n",
    "print(f\"Invalid YearBuilt entries before fix: {((df['YearBuilt'] < 1800) | (df['YearBuilt'] > current_year)).sum()}\")\n",
    "\n",
    "#The fix - replace with NaN\n",
    "df['YearBuilt'] = df['YearBuilt'].where(df['YearBuilt'].between(1800,current_year), np.nan)\n",
    "\n",
    "# 3. Verify\n",
    "print(f\"Invalid YearBuilt entries after fix: {((df['YearBuilt'] < 1800) | (df['YearBuilt'] > current_year)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check count of 0 in bathroom\n",
    "count_bathroom=(df['Bathroom']==0).sum()\n",
    "print(f\"Count of 0 in Bathroom before :{count_bathroom}\")\n",
    "\n",
    "#The fix - replace 0 with NaN\n",
    "df['Bathroom']=df['Bathroom'].replace(0, np.nan)\n",
    "print(f\"Count of 0 in Bathroom after :{(df['Bathroom']==0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c087e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inconsistency Scan\n",
    "text_cols_to_check = df.select_dtypes(include=['object']).columns\n",
    "for col in text_cols_to_check:\n",
    "    if col in df.columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"\\n[{col}] Unique Values ({unique_count}):\")\n",
    "        print(sorted(df[col].unique().astype(str)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inconsistency Fix\n",
    "\n",
    "#Suburb: fix casing\n",
    "df['Suburb'] = df['Suburb'].str.title()\n",
    "\n",
    "#SellerG: remove branch info & fix casing\n",
    "df['SellerG'] = df['SellerG'].str.split('/').str[0].str.strip().str.title()\n",
    "\n",
    "#Verification\n",
    "print(\"--- Suburb ---\")\n",
    "print(sorted(df[df['Suburb'].str.contains('Croydon', case=False)]['Suburb'].unique()))\n",
    "print(sorted(df[df['Suburb'].str.contains('Viewbank', case=False)]['Suburb'].unique()))\n",
    "\n",
    "print(\"\\n--- SellerG ---\")\n",
    "brands_to_check = ['Buxton', 'Hockingstuart', 'Vicprop']\n",
    "for brand in brands_to_check:\n",
    "    print(f\"{brand} variations: {sorted([s for s in df['SellerG'].unique() if brand.lower() in s.lower()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missingness Analysis\n",
    "msno.matrix(df, figsize=(12, 6), sparkline=False);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of missing values per column\n",
    "missing_count = df.isnull().sum()\n",
    "missing_count = missing_count.sort_values(ascending=False)\n",
    "\n",
    "print(\"Count of Missing Values\\n\")\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be926883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where 'Price' is missing (Target)\n",
    "df=df.dropna(subset=['Price'])\n",
    "\n",
    "#Drop rows where 'Postcode' is missing\n",
    "df=df.dropna(subset=['Postcode'])\n",
    "\n",
    "#Verify\n",
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative/Zero Check of numeric columns\n",
    "\n",
    "# 1. Select numeric columns (exclude Lat/Long)\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "cols_to_check = [col for col in numeric_cols if col not in ['Lattitude', 'Longtitude']]\n",
    "\n",
    "print(f\"--- Checking {len(cols_to_check)} numeric columns for Non-Positive values ---\")\n",
    "\n",
    "\n",
    "for col in cols_to_check:\n",
    "    # Count Negatives \n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    \n",
    "    # Count Zeros \n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    \n",
    "    if neg_count > 0 or zero_count > 0:\n",
    "        print(f\"{col:<15} | Negatives: {neg_count:<5} | Zeros: {zero_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fdc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporal Analysis (Time Series)\n",
    "\n",
    "#Extract Year-Month for grouping\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "\n",
    "print(\"ðŸ“… GENERATING TIME SERIES PLOT...\")\n",
    "print(\"   Goal: Check if the market crashed (which affects how we split the data).\")\n",
    "\n",
    "#Aggregate Average Price per Month\n",
    "#Count how many houses sold per month too, to make sure the average is reliable\n",
    "monthly_stats = df.groupby('YearMonth').agg(\n",
    "    Average_Price=('Price', 'mean'),\n",
    "    Count=('Price', 'count')\n",
    ").reset_index()\n",
    "\n",
    "#Convert back to string for plotting\n",
    "monthly_stats['YearMonth'] = monthly_stats['YearMonth'].astype(str)\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=monthly_stats, x='YearMonth', y='Average_Price', marker='o', linewidth=2, color='royalblue')\n",
    "\n",
    "plt.title('Melbourne Housing Market: Price Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Price ($)')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Splitting\n",
    "#Define Features (X) and Target (y)\n",
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "#Random Split\n",
    "#shuffle=True (ensures mix 2016, 2017, and 2018 data together)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "#Verify\n",
    "print(\"--- Random Split Successful ---\")\n",
    "print(f\"Training Set: {X_train.shape[0]} rows\")\n",
    "print(f\"Test Set:     {X_test.shape[0]} rows\")\n",
    "\n",
    "#Safety Check\n",
    "assert len(X_train) + len(X_test) == len(df), \"Error: Row mismatch!\"\n",
    "print(\"Safety Check Passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4adade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cardinality Check\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"\\nCardinality Check (Count of Unique Values)\")\n",
    "cardinality = X_train[categorical_cols].nunique().sort_values(ascending=False)\n",
    "print(cardinality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable Analysis\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Histogram (Distribution)\n",
    "sns.histplot(y_train.dropna(), kde=True, color='blue')\n",
    "plt.title('Price Distribution')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c511523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable Analysis\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Q-Q Plot (Check Normality)\n",
    "ss.probplot(y_train.dropna(), dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable Analysis\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Boxplot (Outliers)\n",
    "sns.boxplot(x=y_train, color='cyan')\n",
    "plt.title('Price Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24243b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Skewness & Kurtosis\n",
    "price_skew = y_train.skew()\n",
    "price_kurt = y_train.kurt()\n",
    "\n",
    "print(\"Target Statistics\")\n",
    "print(f\"Skewness: {price_skew:.4f} (Rule: > 1.0 needs Log Transform)\")\n",
    "print(f\"Kurtosis: {price_kurt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Integrity Check\n",
    "\n",
    "# List of features that should be integers\n",
    "discrete_cols = ['Rooms', 'Bedroom2', 'Bathroom', 'Car', 'Postcode', 'YearBuilt', 'Propertycount']\n",
    "\n",
    "# Check if any value has a non-zero decimal part\n",
    "print(\"Checking for non-integer values (decimals)...\\n\")\n",
    "\n",
    "for col in discrete_cols:\n",
    "    if col in X_train.columns:\n",
    "        # We drop NA first because NaN is technically a float\n",
    "        has_decimals = (X_train[col].dropna() % 1 != 0).any()\n",
    "        \n",
    "        if has_decimals:\n",
    "            print(f\"ALERT: '{col}' contains decimals! \")\n",
    "            examples = X_train[col][X_train[col] % 1 != 0].head(3).tolist()\n",
    "            print(f\"Examples: {examples}\")\n",
    "        else:\n",
    "            print(f\"'{col}' is clean (Integers only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa34490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric Feature Distribution\n",
    "\n",
    "#Select all numeric columns\n",
    "all_numeric = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "#Filter out columns don't need\n",
    "drop_for_plot = ['Price', 'Postcode',] \n",
    "cols_to_plot = [c for c in all_numeric if c not in drop_for_plot]\n",
    "\n",
    "print(f\"Scanning {len(cols_to_plot)} Numeric Features...\\n\")\n",
    "\n",
    "for col in cols_to_plot:\n",
    "    unique_count = X_train[col].nunique()\n",
    "    skew_val = X_train[col].skew()\n",
    "    \n",
    "    #Calculate IQR outlier count\n",
    "    Q1, Q3 = X_train[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_count = ((X_train[col] < Q1 - 1.5*IQR) | (X_train[col] > Q3 + 1.5*IQR)).sum()\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    #Discrete / Count Variables\n",
    "    if unique_count < 25:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(X_train[col].dropna(), discrete=True, kde=True, color='purple')\n",
    "        plt.title(f'{col} (Discrete) | Skew: {skew_val:.2f}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=X_train[col].dropna(), color='cyan')\n",
    "        plt.title(f'{col} Outliers ({outlier_count})')\n",
    "        \n",
    "        #Stats summary for discrete\n",
    "        stats_msg = f\"[{col}] Mode: {X_train[col].mode()[0]} | Skew: {skew_val:.4f} | Outliers: {outlier_count}\"\n",
    "\n",
    "    #Continuous Variables\n",
    "    else:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(X_train[col].dropna(), kde=True, bins=30, color='blue')\n",
    "        plt.title(f'{col} (Continuous) | Skew: {skew_val:.2f}')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=X_train[col].dropna(), color='orange')\n",
    "        plt.title(f'{col} Outliers ({outlier_count})')\n",
    "        \n",
    "        #Stats summary for continuous\n",
    "        stats_msg = f\"[{col}] Min: {X_train[col].min()} | Max: {X_train[col].max()} | Skew: {skew_val:.4f} | Outliers: {outlier_count}\"\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(stats_msg)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a54cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Feature Distributions\n",
    "\n",
    "# Select Categorical Columns\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "#Exclude unwanted features\n",
    "cols_to_ignore = ['Address'] \n",
    "cat_cols = [col for col in cat_cols if col not in cols_to_ignore]\n",
    "\n",
    "#Define High Cardinality Threshold\n",
    "#If a column has > 50 unique values, plotting it is messy.\n",
    "high_cardinality_cutoff = 50 \n",
    "\n",
    "print(f\"Scanning {len(cat_cols)} Categorical Features\\n\")\n",
    "\n",
    "for col in cat_cols:\n",
    "    unique_count = X_train[col].nunique()\n",
    "    \n",
    "    #High cardinality scenario\n",
    "    if unique_count > high_cardinality_cutoff:\n",
    "        print(f\"ANALYZING: {col}\")\n",
    "        print(f\"{col} has {unique_count} unique values (Too many to plot).\")\n",
    "        \n",
    "        # Calculate percentages\n",
    "        top_5_series = X_train[col].value_counts(normalize=True).head(5) * 100\n",
    "        \n",
    "        #Format each number individually to include '%'\n",
    "        print(\"Top 5 Categories:\",top_5_series.apply(lambda x: f\"{x:.2f}%\").to_string())\n",
    "        \n",
    "        #Check for rare labels\n",
    "        freq = X_train[col].value_counts(normalize=True) * 100\n",
    "        rare_count = len(freq[freq < 1.0])\n",
    "        print(f\"Contains {rare_count} rare categories (<1%) that will need encoding.\")\n",
    "        print(\"-\" * 60)\n",
    "        continue\n",
    "\n",
    "    #Low cardinality scenario\n",
    "    print(f\"ANALYZING: {col}\")\n",
    "    print(f\"Unique Categories: {unique_count}\")\n",
    "\n",
    "    #check for rare labels\n",
    "    freq = X_train[col].value_counts(normalize=True) * 100\n",
    "    rare_cats = freq[freq < 1.0] # Threshold: 1%\n",
    "    \n",
    "    if not rare_cats.empty:\n",
    "        print(f\"Rare Labels Found (<1%): {len(rare_cats)}\")\n",
    "        print(f\"Examples: {rare_cats.index.tolist()}\") # Show first 5\n",
    "    else:\n",
    "        print(\"No Rare Labels found.\")\n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize=(10, 6) if unique_count > 10 else (8, 5))\n",
    "    \n",
    "    #Use Horizontal Bars if there are many categories\n",
    "    if unique_count > 10:\n",
    "        sns.countplot(y=X_train[col], order=X_train[col].value_counts().index, palette=\"viridis\")\n",
    "        plt.title(f'{col} Distribution (Horizontal View)')\n",
    "    else:\n",
    "        sns.countplot(x=X_train[col], order=X_train[col].value_counts().index, palette=\"viridis\")\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation & Multicollinearity Check\n",
    "\n",
    "#Select only numeric columns for correlation analysis\n",
    "numeric_df = X_train.select_dtypes(include=['number']).copy()\n",
    "numeric_df = numeric_df.drop(columns=['Price'], errors='ignore')\n",
    "\n",
    "#Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = numeric_df.corr(numeric_only=True)\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap (Numeric Features)')\n",
    "plt.show()\n",
    "\n",
    "#VIF\n",
    "print(\"\\nðŸ”¹ CALCULATING VIF (Multicollinearity Check)...\")\n",
    "print(\"   Rule: VIF > 5 is suspicious. VIF > 10 is definitely redundant.\\n\")\n",
    "\n",
    "#Drop NaNs\n",
    "vif_data = numeric_df.dropna()\n",
    "\n",
    "#Add constant\n",
    "vif_data = add_constant(vif_data)\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"Feature\"] = vif_data.columns\n",
    "vif_df[\"VIF_Score\"] = [\n",
    "    variance_inflation_factor(vif_data.values, i)\n",
    "    for i in range(vif_data.shape[1])\n",
    "]\n",
    "\n",
    "#Drop const row\n",
    "vif_df = vif_df[vif_df[\"Feature\"] != \"const\"]\n",
    "\n",
    "#Show sorted output\n",
    "print(vif_df.sort_values(by=\"VIF_Score\", ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db805e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical vs Categorical (Redundancy Check)\n",
    "\n",
    "#Define Categorical Columns to Check\n",
    "#Avoid extremely high-cardinality columns\n",
    "available_cols = X_train.columns.tolist()\n",
    "target_cols = ['Suburb', 'Type', 'Method', 'Regionname', 'CouncilArea']\n",
    "\n",
    "#\n",
    "cat_features = [col for col in target_cols if col in available_cols]\n",
    "\n",
    "#Cramer's V Function\n",
    "def cramers_v(x, y):\n",
    "    #Case 1: Same column â†’ Perfect association\n",
    "    if x.name == y.name:\n",
    "        return 1.0\n",
    "\n",
    "    #Case 2: A column with only 1 unique category â†’ No association possible\n",
    "    if x.nunique() < 2 or y.nunique() < 2:\n",
    "        return 0.0\n",
    "\n",
    "    #Contingency table\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "\n",
    "    #Chi-square\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "\n",
    "    #Bias correction\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "        rcorr = r - ((r - 1)**2) / (n - 1)\n",
    "        kcorr = k - ((k - 1)**2) / (n - 1)\n",
    "\n",
    "        denom = min((kcorr - 1), (rcorr - 1))\n",
    "        if denom <= 0:\n",
    "            return 0.0\n",
    "        return np.sqrt(phi2corr / denom)\n",
    "\n",
    "#Calculate Cramer's V Matrix\n",
    "print(\"CALCULATING CATEGORICAL ASSOCIATIONS (Cramer's V)...\")\n",
    "print(\"   Goal: Find redundant categorical features (Score > 0.80 means redundancy).\\n\")\n",
    "\n",
    "rows = []\n",
    "for var1 in cat_features:\n",
    "    col_values = []\n",
    "    for var2 in cat_features:\n",
    "        v = cramers_v(X_train[var1], X_train[var2])\n",
    "        col_values.append(v)\n",
    "    rows.append(col_values)\n",
    "\n",
    "cramers_df = pd.DataFrame(rows, columns=cat_features, index=cat_features)\n",
    "cramers_df.fillna(0, inplace=True)\n",
    "\n",
    "#Plot the Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_df, annot=True, fmt=\".2f\", cmap='Greens', vmin=0, vmax=1)\n",
    "plt.title(\"Categorical Association Heatmap (Cramer's V)\")\n",
    "plt.show()\n",
    "\n",
    "# Print Table\n",
    "print(\"\\nCramer's V Association Table:\")\n",
    "print(cramers_df)\n",
    "\n",
    "#Identify Highly Associated Pairs\n",
    "threshold = 0.8\n",
    "print(\"\\nHighly Associated Categorical Pairs (Cramer's V > 0.8):\")\n",
    "for i in range(len(cat_features)):\n",
    "    for j in range(i+1, len(cat_features)):\n",
    "        if cramers_df.iloc[i, j] > threshold:\n",
    "            print(f\"{cat_features[i]} â†” {cat_features[j]} : {cramers_df.iloc[i, j]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08081587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric vs Target (relationship between the target and numerical features)\n",
    "\n",
    "#Create a temp dataframe combining X and y for plotting\n",
    "#Use a copy so it don't accidentally modify the real X_train\n",
    "plot_df = X_train.copy()\n",
    "plot_df['Price'] = y_train\n",
    "\n",
    "#Numeric features to check\n",
    "features_to_plot = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt', 'Car', 'Propertycount']\n",
    "\n",
    "print(f\"Generating Individual Scatter Plots for {len(features_to_plot)} features vs Price...\\n\")\n",
    "\n",
    "for col in features_to_plot:\n",
    "    #Check if column exists\n",
    "    if col not in plot_df.columns: continue\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    #Plot with hue='Type' to see if Houses (h) behave differently than Units (u)\n",
    "    sns.scatterplot(\n",
    "        data=plot_df.sample(n=min(len(plot_df), 2000), random_state=42), #Sample 2000 points for speed/clarity\n",
    "        x=col, \n",
    "        y='Price', \n",
    "        hue='Type', \n",
    "        alpha=0.6, \n",
    "        palette='Set1'\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Price vs {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9896d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical vs Target (relationship between the target and categorical features)\n",
    "\n",
    "#Define the categorical features to check\n",
    "#Stick to the manageable ones (Suburb/Council are too big to plot here)\n",
    "plot_df = X_train.copy()\n",
    "plot_df['Price'] = y_train\n",
    "\n",
    "cat_cols = ['Type', 'Method', 'Regionname'] \n",
    "target = 'Price'\n",
    "\n",
    "print(\"GENERATING BOXPLOTS...\")\n",
    "print(\"Goal: Look for categories that separate the price (boxes at different heights).\\n\")\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col not in plot_df.columns: continue\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    #Sort the order by Median Price so the chart is easy to read (Low -> High)\n",
    "    order = plot_df.groupby(col)[target].median().sort_values().index\n",
    "    \n",
    "    #Create the boxplot\n",
    "    sns.boxplot(data=plot_df, x=col, y=target, order=order, palette='coolwarm')\n",
    "    \n",
    "    plt.title(f'{col} vs Price Distribution')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric vs Categorical (Redundancy Check)\n",
    "\n",
    "#Select all numeric columns\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "#Select only Low cardinality categorical columns\n",
    "categorical_cols = [col for col in X_train.select_dtypes(include=['object', 'category']).columns \n",
    "                    if X_train[col].nunique() < 20]\n",
    "\n",
    "print(f\"Analyzing redundancy between:\")\n",
    "print(f\"Numeric: {list(numeric_cols)}\")\n",
    "print(f\"Categorical: {categorical_cols}\")\n",
    "\n",
    "#Prepare a temporary dataframe\n",
    "X_check = X_train.copy()\n",
    "\n",
    "#Encode categorical to numbers (MI requires numbers)\n",
    "for col in categorical_cols:\n",
    "    X_check[col] = LabelEncoder().fit_transform(X_check[col].astype(str))\n",
    "\n",
    "#Impute missing numeric values (MI cannot handle NaNs)\n",
    "for col in numeric_cols:\n",
    "    X_check[col] = X_check[col].fillna(X_check[col].median())\n",
    "\n",
    "#Compute Mutual Information\n",
    "mi_results = pd.DataFrame(index=numeric_cols, columns=categorical_cols, dtype=float)\n",
    "\n",
    "for cat_col in categorical_cols:\n",
    "    mi = mutual_info_classif(\n",
    "        X_check[numeric_cols],   # Predictors(The Numerical features)\n",
    "        X_check[cat_col],        # Target(The Categorical feature)\n",
    "        discrete_features=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    mi_results[cat_col] = mi\n",
    "\n",
    "#Interpret Results\n",
    "threshold = 0.15 \n",
    "\n",
    "redundant_pairs = []\n",
    "for num in numeric_cols:\n",
    "    for cat in categorical_cols:\n",
    "        score = mi_results.loc[num, cat]\n",
    "        if score > threshold:\n",
    "            redundant_pairs.append((num, cat, score))\n",
    "\n",
    "#Sort by strongest redundancy first\n",
    "redundant_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f\"\\nAnalysis Complete. Found {len(redundant_pairs)} potential redundancies:\")\n",
    "for num, cat, score in redundant_pairs:\n",
    "    print(f\"{num} â†” {cat} : MI Score = {score:.3f}\")\n",
    "\n",
    "#Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(mi_results, annot=True, cmap='Blues', fmt=\".2f\")\n",
    "plt.title(\"Mutual Information: Which Numeric Features Predict Categorical Features?\")\n",
    "plt.xlabel(\"Categorical Features\")\n",
    "plt.ylabel(\"Numeric Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
