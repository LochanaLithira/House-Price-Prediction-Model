{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import scipy.stats as ss\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dde6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Settings\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', None)  #This displays all columns in the dataframe\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)  # Display describe() with normal float formatting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff194d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/raw/Melbourne_housing_FULL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207228b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b414af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structure of the dataset\n",
    "rows=df.shape[0]\n",
    "columns=df.shape[1]\n",
    "print(\"Number of rows:\", rows )\n",
    "print(\"Number of columns:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Types & Missing Counts\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime object\n",
    "# 'dayfirst=True' is critical because the dataset uses DD/MM/YYYY format\n",
    "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "\n",
    "# Verification\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate check(rows)\n",
    "duplicates=df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicates)\n",
    "\n",
    "# Show duplicated rows\n",
    "df[df.duplicated()]\n",
    "\n",
    "#Remove duplicates\n",
    "df=df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "#Verify\n",
    "print(f\"Duplicates after: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad789ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many impossible BuildingArea values are there (<5.0)\n",
    "print(f\"Count of Impossible BuildingArea values: {len(df[df['BuildingArea'] < 5])}\")\n",
    "\n",
    "# The Fix: Replace values < 5.0 with NaN (Missing) \n",
    "df.loc[df['BuildingArea'] < 5, 'BuildingArea'] = np.nan\n",
    "\n",
    "print(f\"Count of Impossible BuildingArea values after fix:{len(df[df['BuildingArea'] < 5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d87518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dynamic current year\n",
    "current_year=datetime.datetime.now().year\n",
    "\n",
    "#Check count of invalid years before fix\n",
    "print(f\"Invalid YearBuilt entries before fix: {((df['YearBuilt'] < 1800) | (df['YearBuilt'] > current_year)).sum()}\")\n",
    "\n",
    "#The fix - replace with NaN\n",
    "df['YearBuilt'] = df['YearBuilt'].where(df['YearBuilt'].between(1800,current_year), np.nan)\n",
    "\n",
    "# 3. Verify\n",
    "print(f\"Invalid YearBuilt entries after fix: {((df['YearBuilt'] < 1800) | (df['YearBuilt'] > current_year)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check count of 0 in bathroom\n",
    "count_bathroom=(df['Bathroom']==0).sum()\n",
    "print(f\"Count of 0 in Bathroom before :{count_bathroom}\")\n",
    "df['Bathroom']=df['Bathroom'].replace(0, np.nan)\n",
    "print(f\"Count of 0 in Bathroom after :{(df['Bathroom']==0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c087e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inconsistency Scan\n",
    "text_cols_to_check = df.select_dtypes(include=['object']).columns\n",
    "for col in text_cols_to_check:\n",
    "    if col in df.columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"\\n[{col}] Unique Values ({unique_count}):\")\n",
    "        print(sorted(df[col].unique().astype(str)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inconsistency Fix (Applied BEFORE split - safe deterministic transformations)\n",
    "\n",
    "# Suburb: fix casing\n",
    "df['Suburb'] = df['Suburb'].str.title()\n",
    "\n",
    "# SellerG: remove branch info & fix casing\n",
    "df['SellerG'] = df['SellerG'].str.split('/').str[0].str.strip().str.title()\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"--- Suburb ---\")\n",
    "print(sorted(df[df['Suburb'].str.contains('Croydon', case=False)]['Suburb'].unique()))\n",
    "print(sorted(df[df['Suburb'].str.contains('Viewbank', case=False)]['Suburb'].unique()))\n",
    "\n",
    "print(\"\\n--- SellerG ---\")\n",
    "brands_to_check = ['Buxton', 'Hockingstuart', 'Vicprop']\n",
    "for brand in brands_to_check:\n",
    "    print(f\"{brand} variations: {sorted([s for s in df['SellerG'].unique() if brand.lower() in s.lower()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missingness Analysis\n",
    "msno.matrix(df, figsize=(12, 6), sparkline=False);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of missing values per column\n",
    "missing_count = df.isnull().sum()\n",
    "missing_count = missing_count.sort_values(ascending=False)\n",
    "\n",
    "print(\"Count of Missing Values\\n\")\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be926883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where 'Price' is missing (Target)\n",
    "df=df.dropna(subset=['Price'])\n",
    "\n",
    "#Drop rows where 'Postcode' is missing\n",
    "df=df.dropna(subset=['Postcode'])\n",
    "\n",
    "#Verify\n",
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative/Zero Check of numeric columns\n",
    "\n",
    "# 1. Select numeric columns but exclude Lat/Long (which can be negative)\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "cols_to_check = [col for col in numeric_cols if col not in ['Lattitude', 'Longtitude']]\n",
    "\n",
    "print(f\"--- Checking {len(cols_to_check)} numeric columns for Non-Positive values ---\")\n",
    "\n",
    "# 2. Loop through and report issues\n",
    "for col in cols_to_check:\n",
    "    # Count Negatives (Usually Errors for these columns)\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    \n",
    "    # Count Zeros (Context dependent: Bad for Price, OK for Car)\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    \n",
    "    if neg_count > 0 or zero_count > 0:\n",
    "        print(f\"‚ö†Ô∏è {col:<15} | Negatives: {neg_count:<5} | Zeros: {zero_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fdc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Temporal Analysis (Time Series) ---\n",
    "\n",
    "# Note: Date column was already converted to datetime in Cell 7 using dayfirst=True\n",
    "# No need to convert again - just extract Year-Month for grouping\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "\n",
    "print(\"üìÖ GENERATING TIME SERIES PLOT...\")\n",
    "print(\"   Goal: Check if the market crashed (which affects how we split the data).\")\n",
    "\n",
    "# 3. Aggregate Average Price per Month\n",
    "# We count how many houses sold per month too, to make sure the average is reliable\n",
    "monthly_stats = df.groupby('YearMonth').agg(\n",
    "    Average_Price=('Price', 'mean'),\n",
    "    Count=('Price', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Convert back to string for plotting\n",
    "monthly_stats['YearMonth'] = monthly_stats['YearMonth'].astype(str)\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=monthly_stats, x='YearMonth', y='Average_Price', marker='o', linewidth=2, color='royalblue')\n",
    "\n",
    "plt.title('Melbourne Housing Market: Price Trend Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Price ($)')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Data Splitting Strategy ---\n",
    "# 1. Define Features (X) and Target (y)\n",
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "# 2. Perform Random Split (User Decision: Random Split for Generalization)\n",
    "# shuffle=True ensures we mix 2016, 2017, and 2018 data together\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Verify\n",
    "print(\"--- Random Split Successful ---\")\n",
    "print(f\"Training Set: {X_train.shape[0]} rows\")\n",
    "print(f\"Test Set:     {X_test.shape[0]} rows\")\n",
    "\n",
    "# 4. SAFETY CHECK\n",
    "assert len(X_train) + len(X_test) == len(df), \"Error: Row mismatch!\"\n",
    "print(\"Safety Check Passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4adade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cardinality Check\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"\\nCardinality Check (Count of Unique Values)\")\n",
    "cardinality = X_train[categorical_cols].nunique().sort_values(ascending=False)\n",
    "print(cardinality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable Analysis\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Histogram (Distribution)\n",
    "sns.histplot(y_train.dropna(), kde=True, color='blue')\n",
    "plt.title('Price Distribution')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c511523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable Analysis\n",
    "plt.figure(figsize=(14, 5))\n",
    "# Plot Q-Q Plot (Check Normality)\n",
    "ss.probplot(y_train.dropna(), dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable Analysis\n",
    "plt.figure(figsize=(14, 5))\n",
    "# Plot Boxplot (Outliers)\n",
    "\n",
    "sns.boxplot(x=y_train, color='cyan')\n",
    "plt.title('Price Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24243b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Skewness & Kurtosis\n",
    "price_skew = y_train.skew()\n",
    "price_kurt = y_train.kurt()\n",
    "\n",
    "print(\"Target Statistics\")\n",
    "print(f\"Skewness: {price_skew:.4f} (Rule: > 1.0 needs Log Transform)\")\n",
    "print(f\"Kurtosis: {price_kurt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Integrity Check: Are there decimals in Discrete variables? ---\n",
    "\n",
    "# List of columns that SHOULD be integers\n",
    "discrete_cols = ['Rooms', 'Bedroom2', 'Bathroom', 'Car', 'Postcode', 'YearBuilt', 'Propertycount']\n",
    "\n",
    "print(\"Checking for non-integer values (decimals)...\\n\")\n",
    "\n",
    "for col in discrete_cols:\n",
    "    if col in X_train.columns:\n",
    "        # Check if any value has a non-zero decimal part (e.g. 2.5)\n",
    "        # We drop NA first because NaN is technically a float\n",
    "        has_decimals = (X_train[col].dropna() % 1 != 0).any()\n",
    "        \n",
    "        if has_decimals:\n",
    "            print(f\"ALERT: '{col}' contains decimals! (e.g., 2.5)\")\n",
    "            # Show examples\n",
    "            examples = X_train[col][X_train[col] % 1 != 0].head(3).tolist()\n",
    "            print(f\"   Examples: {examples}\")\n",
    "        else:\n",
    "            print(f\"'{col}' is clean (Integers only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa34490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Smart Numeric Analysis: Distribution + Outliers + Skewness ---\n",
    "\n",
    "# 1. Select all numeric columns\n",
    "all_numeric = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# 2. Filter out columns we don't need\n",
    "drop_for_plot = ['Price', 'Postcode',] \n",
    "cols_to_plot = [c for c in all_numeric if c not in drop_for_plot]\n",
    "\n",
    "print(f\"Scanning {len(cols_to_plot)} Numeric Features...\\n\")\n",
    "\n",
    "# 3. The Smart Loop\n",
    "for col in cols_to_plot:\n",
    "    unique_count = X_train[col].nunique()\n",
    "    skew_val = X_train[col].skew()\n",
    "    \n",
    "    # Calculate IQR outlier count\n",
    "    Q1, Q3 = X_train[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_count = ((X_train[col] < Q1 - 1.5*IQR) | (X_train[col] > Q3 + 1.5*IQR)).sum()\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # --- BRANCH A: Discrete / Count Variables (e.g., Rooms, Car) ---\n",
    "    if unique_count < 25:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(X_train[col].dropna(), discrete=True, kde=True, color='purple')\n",
    "        plt.title(f'{col} (Discrete) | Skew: {skew_val:.2f}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=X_train[col].dropna(), color='cyan')\n",
    "        plt.title(f'{col} Outliers ({outlier_count})')\n",
    "        \n",
    "        # Stats summary for discrete\n",
    "        stats_msg = f\"[{col}] Mode: {X_train[col].mode()[0]} | Skew: {skew_val:.4f} | Outliers: {outlier_count}\"\n",
    "    # --- BRANCH B: Continuous Variables (e.g., Landsize, Distance) ---\n",
    "    else:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(X_train[col].dropna(), kde=True, bins=30, color='blue')\n",
    "        plt.title(f'{col} (Continuous) | Skew: {skew_val:.2f}')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=X_train[col].dropna(), color='orange')\n",
    "        plt.title(f'{col} Outliers ({outlier_count})')\n",
    "        \n",
    "        # Stats summary for continuous\n",
    "        stats_msg = f\"[{col}] Min: {X_train[col].min()} | Max: {X_train[col].max()} | Skew: {skew_val:.4f} | Outliers: {outlier_count}\"\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(stats_msg)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a54cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Categorical Feature Distributions (Automated & Robust) ---\n",
    "\n",
    "# 1. Auto-Select Categorical Columns\n",
    "# We grab all object columns, but we EXCLUDE 'Address' and 'Date' \n",
    "# because they are unique identifiers/time series, not categories.\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "cols_to_ignore = ['Address'] \n",
    "cat_cols = [col for col in cat_cols if col not in cols_to_ignore]\n",
    "\n",
    "# 2. Define High Cardinality Threshold\n",
    "# If a column has > 50 unique values, plotting it is messy. We will just print stats.\n",
    "high_cardinality_cutoff = 50 \n",
    "\n",
    "print(f\"Scanning {len(cat_cols)} Categorical Features\\n\")\n",
    "\n",
    "for col in cat_cols:\n",
    "    unique_count = X_train[col].nunique()\n",
    "    # --- STRATEGY A: HIGH CARDINALITY (e.g., Suburb, SellerG) ---\n",
    "    if unique_count > high_cardinality_cutoff:\n",
    "        print(f\"ANALYZING: {col}\")\n",
    "        print(f\"{col} has {unique_count} unique values (Too many to plot).\")\n",
    "        \n",
    "        # Calculate percentages\n",
    "        top_5_series = X_train[col].value_counts(normalize=True).head(5) * 100\n",
    "        \n",
    "        # Fix: Format EACH number individually to include '%'\n",
    "        print(\"Top 5 Categories:\",top_5_series.apply(lambda x: f\"{x:.2f}%\").to_string())\n",
    "        \n",
    "        # Quick check for rare labels\n",
    "        freq = X_train[col].value_counts(normalize=True) * 100\n",
    "        rare_count = len(freq[freq < 1.0])\n",
    "        print(f\"Contains {rare_count} rare categories (<1%) that will need encoding.\")\n",
    "        print(\"-\" * 60)\n",
    "        continue\n",
    "    # --- STRATEGY B: PLOT-ABLE FEATURES (e.g., Type, Method, CouncilArea) ---\n",
    "    print(f\"ANALYZING: {col}\")\n",
    "    print(f\"Unique Categories: {unique_count}\")\n",
    "\n",
    "    # 1. Rare Label Check (< 1%)\n",
    "    freq = X_train[col].value_counts(normalize=True) * 100\n",
    "    rare_cats = freq[freq < 1.0] # Threshold: 1%\n",
    "    \n",
    "    if not rare_cats.empty:\n",
    "        print(f\"Rare Labels Found (<1%): {len(rare_cats)}\")\n",
    "        print(f\"Examples: {rare_cats.index.tolist()}\") # Show first 5\n",
    "    else:\n",
    "        print(\"No Rare Labels found.\")\n",
    "\n",
    "    # 2. Visualization\n",
    "    plt.figure(figsize=(10, 6) if unique_count > 10 else (8, 5))\n",
    "    \n",
    "    # Logic: Use Horizontal Bars if there are many categories (like CouncilArea)\n",
    "    if unique_count > 10:\n",
    "        sns.countplot(y=X_train[col], order=X_train[col].value_counts().index, palette=\"viridis\")\n",
    "        plt.title(f'{col} Distribution (Horizontal View)')\n",
    "    else:\n",
    "        sns.countplot(x=X_train[col], order=X_train[col].value_counts().index, palette=\"viridis\")\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Correlation & Multicollinearity Check ---\n",
    "\n",
    "# Select only numeric columns for correlation analysis\n",
    "numeric_df = X_train.select_dtypes(include=['number']).copy()\n",
    "numeric_df = numeric_df.drop(columns=['Price'], errors='ignore')\n",
    "\n",
    "# A. Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = numeric_df.corr(numeric_only=True)\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap (Numeric Features)')\n",
    "plt.show()\n",
    "\n",
    "# B. VIF\n",
    "print(\"\\nüîπ CALCULATING VIF (Multicollinearity Check)...\")\n",
    "print(\"   Rule: VIF > 5 is suspicious. VIF > 10 is definitely redundant.\\n\")\n",
    "\n",
    "# Drop NaNs\n",
    "vif_data = numeric_df.dropna()\n",
    "\n",
    "# Add constant\n",
    "vif_data = add_constant(vif_data)\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"Feature\"] = vif_data.columns\n",
    "vif_df[\"VIF_Score\"] = [\n",
    "    variance_inflation_factor(vif_data.values, i)\n",
    "    for i in range(vif_data.shape[1])\n",
    "]\n",
    "\n",
    "# Drop const row\n",
    "vif_df = vif_df[vif_df[\"Feature\"] != \"const\"]\n",
    "\n",
    "# Show sorted output\n",
    "print(vif_df.sort_values(by=\"VIF_Score\", ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db805e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Categorical vs Categorical (Redundancy Check)\n",
    "# ===============================\n",
    "\n",
    "# 1. Define Categorical Columns to Check\n",
    "# Avoid extremely high-cardinality columns (e.g., Suburb, SellerG)\n",
    "available_cols = X_train.columns.tolist()\n",
    "target_cols = ['Suburb', 'Type', 'Method', 'Regionname', 'CouncilArea']\n",
    "\n",
    "# Filter to ensure we don't crash if a column was dropped earlier\n",
    "cat_features = [col for col in target_cols if col in available_cols]\n",
    "# ---------- Cramer's V Function (Safe & Correct) ----------\n",
    "def cramers_v(x, y):\n",
    "    # Case 1: Same column ‚Üí Perfect association\n",
    "    if x.name == y.name:\n",
    "        return 1.0\n",
    "\n",
    "    # Case 2: A column with only 1 unique category ‚Üí No association possible\n",
    "    if x.nunique() < 2 or y.nunique() < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Contingency table\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "\n",
    "    # Chi-square\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "\n",
    "    # Bias correction\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "        rcorr = r - ((r - 1)**2) / (n - 1)\n",
    "        kcorr = k - ((k - 1)**2) / (n - 1)\n",
    "\n",
    "        denom = min((kcorr - 1), (rcorr - 1))\n",
    "        if denom <= 0:\n",
    "            return 0.0\n",
    "        return np.sqrt(phi2corr / denom)\n",
    "\n",
    "# ---------- Calculate Cramer's V Matrix ----------\n",
    "print(\"üîÑ CALCULATING CATEGORICAL ASSOCIATIONS (Cramer's V)...\")\n",
    "print(\"   Goal: Find redundant categorical features (Score > 0.80 means redundancy).\\n\")\n",
    "\n",
    "rows = []\n",
    "for var1 in cat_features:\n",
    "    col_values = []\n",
    "    for var2 in cat_features:\n",
    "        v = cramers_v(X_train[var1], X_train[var2])\n",
    "        col_values.append(v)\n",
    "    rows.append(col_values)\n",
    "\n",
    "cramers_df = pd.DataFrame(rows, columns=cat_features, index=cat_features)\n",
    "cramers_df.fillna(0, inplace=True)\n",
    "\n",
    "# ---------- Plot the Heatmap ----------\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_df, annot=True, fmt=\".2f\", cmap='Greens', vmin=0, vmax=1)\n",
    "plt.title(\"Categorical Association Heatmap (Cramer's V)\")\n",
    "plt.show()\n",
    "\n",
    "# Print Table\n",
    "print(\"\\nCramer's V Association Table:\")\n",
    "print(cramers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08081587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Numeric vs Target (Individual Scatter Plots) ---\n",
    "\n",
    "# Create a temp dataframe combining X and y for plotting\n",
    "# We use a copy so we don't accidentally modify the real X_train\n",
    "plot_df = X_train.copy()\n",
    "plot_df['Price'] = y_train\n",
    "\n",
    "# Select key numeric features to check\n",
    "features_to_plot = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt', 'Car', 'Propertycount']\n",
    "\n",
    "print(f\"üìä Generating Individual Scatter Plots for {len(features_to_plot)} features vs Price...\\n\")\n",
    "\n",
    "for col in features_to_plot:\n",
    "    # Check if column exists (just in case you dropped one earlier)\n",
    "    if col not in plot_df.columns: continue\n",
    "\n",
    "    plt.figure(figsize=(10, 6)) # Large, clear size for each plot\n",
    "    \n",
    "    # Plot with hue='Type' to see if Houses (h) behave differently than Units (u)\n",
    "    sns.scatterplot(\n",
    "        data=plot_df.sample(n=min(len(plot_df), 2000), random_state=42), # Sample 2000 points for speed/clarity\n",
    "        x=col, \n",
    "        y='Price', \n",
    "        hue='Type', \n",
    "        alpha=0.6, \n",
    "        palette='Set1'\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Price vs {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.grid(True, alpha=0.3) # Add gridlines for readability\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9896d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Categorical vs Target (Boxplots) ---\n",
    "\n",
    "# Define the categorical features to check\n",
    "# We stick to the manageable ones (Suburb/Council are too big to plot here)\n",
    "plot_df = X_train.copy()\n",
    "plot_df['Price'] = y_train\n",
    "\n",
    "cat_cols = ['Type', 'Method', 'Regionname'] \n",
    "target = 'Price'\n",
    "\n",
    "print(\"GENERATING BOXPLOTS...\")\n",
    "print(\"   Goal: Look for categories that separate the price (boxes at different heights).\\n\")\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col not in plot_df.columns: continue\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Sort the order by Median Price so the chart is easy to read (Low -> High)\n",
    "    order = plot_df.groupby(col)[target].median().sort_values().index\n",
    "    \n",
    "    # Create the boxplot\n",
    "    sns.boxplot(data=plot_df, x=col, y=target, order=order, palette='coolwarm')\n",
    "    \n",
    "    plt.title(f'{col} vs Price Distribution')\n",
    "    plt.xticks(rotation=45, ha='right') # Rotate labels for readability\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# --- Step 1: Encode categorical features to integers ---\n",
    "X_train_encoded = X_train.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_encoded[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    label_encoders[col] = le  # store for later use if needed\n",
    "\n",
    "# --- Step 2: Impute missing numeric values (median) ---\n",
    "for col in numeric_cols:\n",
    "    X_train_encoded[col] = X_train_encoded[col].fillna(X_train_encoded[col].median())\n",
    "\n",
    "# --- Step 3: Compute Mutual Information ---\n",
    "mi_results = pd.DataFrame(index=numeric_cols, columns=categorical_cols)\n",
    "\n",
    "for cat_col in categorical_cols:\n",
    "    mi = mutual_info_classif(\n",
    "        X_train_encoded[numeric_cols],   # predictors (numeric)\n",
    "        X_train_encoded[cat_col],        # categorical \"target\"\n",
    "        discrete_features=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    mi_results[cat_col] = mi\n",
    "\n",
    "print(mi_results)\n",
    "\n",
    "# --- Step 4: Interpret results ---\n",
    "# Set a threshold for high MI indicating redundancy (example: 0.1)\n",
    "threshold = 0.5\n",
    "redundant_numeric_features = mi_results.index[(mi_results > threshold).any(axis=1)].tolist()\n",
    "\n",
    "print(\"‚úÖ Mutual Information computed for numeric vs categorical features\")\n",
    "print(\"Potentially redundant numeric features based on MI > threshold:\")\n",
    "print(redundant_numeric_features)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(mi_results.astype(float), annot=True, cmap='Blues')\n",
    "plt.title(\"Mutual Information (Numeric vs Categorical)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3077d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
