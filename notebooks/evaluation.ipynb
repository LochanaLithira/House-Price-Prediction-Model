{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD DATA & MODEL ---\n",
    "data_path = '../data/processed/'\n",
    "model_path = '../models/house_price_model.joblib'\n",
    "\n",
    "print(\"â³ Loading data and model...\")\n",
    "\n",
    "# Load Test Features\n",
    "X_test = pd.read_parquet(f'{data_path}X_test_processed.parquet')\n",
    "\n",
    "# Load Test Targets (Log Scale)\n",
    "y_test_log = pd.read_parquet(f'{data_path}y_test_log.parquet')['Price']\n",
    "\n",
    "# Convert to Real Dollars\n",
    "y_test_real = np.expm1(y_test_log)\n",
    "\n",
    "# Load Model\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "print(f\"âœ… Loaded Model: {type(model).__name__}\")\n",
    "print(f\"   Test Data Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GENERATE PREDICTIONS ---\n",
    "print(\"ðŸ”® Generating predictions...\")\n",
    "\n",
    "# Predict (Log Scale)\n",
    "y_pred_log = model.predict(X_test)\n",
    "\n",
    "# Convert to Real Dollars\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test_real,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "# Calculate Residuals (Error)\n",
    "results_df['Error'] = results_df['Actual'] - results_df['Predicted']\n",
    "results_df['Abs_Error'] = results_df['Error'].abs()\n",
    "\n",
    "print(\"âœ… Predictions complete.\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbca4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- METRICS ---\n",
    "mae = mean_absolute_error(y_test_real, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_real, y_pred))\n",
    "r2 = r2_score(y_test_real, y_pred)\n",
    "\n",
    "print(\"ðŸ† FINAL TEST METRICS ðŸ†\")\n",
    "print(f\"   MAE:  ${mae:,.0f}\")\n",
    "print(f\"   RMSE: ${rmse:,.0f}\")\n",
    "print(f\"   R2:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION 1: ACTUAL vs PREDICTED ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=results_df, x='Actual', y='Predicted', alpha=0.5, color='blue', edgecolor=None)\n",
    "\n",
    "# Perfect prediction line\n",
    "plt.plot([results_df['Actual'].min(), results_df['Actual'].max()], \n",
    "         [results_df['Actual'].min(), results_df['Actual'].max()], \n",
    "         'r--', lw=3, label='Perfect Prediction')\n",
    "\n",
    "plt.title(\"Actual vs Predicted Prices\")\n",
    "plt.xlabel(\"Actual Price ($)\")\n",
    "plt.ylabel(\"Predicted Price ($)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION 2: RESIDUAL PLOT ---\n",
    "# Check for patterns in errors (Heteroscedasticity)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=results_df, x='Predicted', y='Error', alpha=0.5, color='purple', edgecolor=None)\n",
    "plt.axhline(0, color='red', linestyle='--', lw=2)\n",
    "\n",
    "plt.title(\"Residual Plot (Predicted vs Error)\")\n",
    "plt.xlabel(\"Predicted Price ($)\")\n",
    "plt.ylabel(\"Error (Actual - Predicted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0869646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION 3: ERROR DISTRIBUTION ---\n",
    "# Are errors normally distributed?\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['Error'], bins=50, kde=True, color='orange')\n",
    "plt.title(\"Distribution of Prediction Errors\")\n",
    "plt.xlabel(\"Error ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION 4: FEATURE IMPORTANCE ---\n",
    "# Check if model supports feature importance\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    importances = model.feature_importances_\n",
    "    features = X_test.columns\n",
    "    \n",
    "    feat_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "    feat_df = feat_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feat_df, y='Feature', x='Importance', palette='magma')\n",
    "    plt.title(\"Top 20 Features Driving House Prices\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model does not support feature importance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96265887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSIS: WORST PREDICTIONS ---\n",
    "print(\"\\nðŸ“‰ TOP 10 WORST PREDICTIONS (High Error)\")\n",
    "worst_predictions = results_df.sort_values(by='Abs_Error', ascending=False).head(10)\n",
    "\n",
    "# Format for easier reading\n",
    "display(worst_predictions.style.format({\n",
    "    'Actual': '${:,.0f}', \n",
    "    'Predicted': '${:,.0f}', \n",
    "    'Error': '${:,.0f}',\n",
    "    'Abs_Error': '${:,.0f}'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48304bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEEP DIVE: DIAGNOSING THE WORST PREDICTIONS ---\n",
    "# We need to merge the \"Worst Predictions\" back with their original features (X_test)\n",
    "# to see WHY they failed (e.g., Are they all Mansions? Do they have missing BuildingArea?)\n",
    "\n",
    "# 1. Get the indices of the worst predictions\n",
    "worst_indices = worst_predictions.index\n",
    "\n",
    "# 2. Fetch their feature values from X_test\n",
    "# Note: X_test is already processed (scaled/encoded), so it might be hard to read.\n",
    "# Ideally, we would look at the raw data, but we can still check key indicators.\n",
    "worst_combined = X_test.loc[worst_indices].copy()\n",
    "\n",
    "# 3. Add the Prediction info back\n",
    "worst_combined['Actual_Price'] = worst_predictions['Actual']\n",
    "worst_combined['Predicted_Price'] = worst_predictions['Predicted']\n",
    "worst_combined['Error'] = worst_predictions['Error']\n",
    "\n",
    "# 4. Check for \"Luxury Bias\"\n",
    "# If Actual Price is consistently > $3M, our model struggles with luxury homes.\n",
    "print(\"ðŸ’° Price Analysis of Failures:\")\n",
    "print(worst_combined[['Actual_Price', 'Predicted_Price', 'Error']].describe())\n",
    "\n",
    "# 5. Check for \"Imputation Artifacts\"\n",
    "# If 'BuildingArea' or 'YearBuilt' are exactly the median/mode values we used for imputation,\n",
    "# it means we predicted based on guessed data.\n",
    "# (We need to check the raw values, but let's look for patterns first)\n",
    "\n",
    "print(\"\\nðŸ” Feature Analysis of Failures (Mean values vs Global Mean):\")\n",
    "# Compare the \"Worst\" rows vs the \"Average\" house in X_test\n",
    "comparison = pd.DataFrame({\n",
    "    'Worst_10_Mean': worst_combined.mean(numeric_only=True),\n",
    "    'Global_Test_Mean': X_test.mean(numeric_only=True)\n",
    "})\n",
    "display(comparison.loc[['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea']])\n",
    "\n",
    "# 6. Visual Check\n",
    "# Are we under-predicting or over-predicting?\n",
    "under_estimated = (worst_combined['Error'] > 0).sum()\n",
    "over_estimated = (worst_combined['Error'] < 0).sum()\n",
    "\n",
    "print(f\"\\nðŸ“‰ Diagnosis:\")\n",
    "print(f\"   Under-Estimated (Real Price was Higher): {under_estimated}\")\n",
    "print(f\"   Over-Estimated (Real Price was Lower):   {over_estimated}\")\n",
    "\n",
    "if under_estimated > over_estimated:\n",
    "    print(\"   ðŸ‘‰ CONCLUSION: The model tends to UNDER-value these outliers (likely Luxury homes).\")\n",
    "else:\n",
    "    print(\"   ðŸ‘‰ CONCLUSION: The model tends to OVER-value these outliers (likely Dump/Teardown homes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86388d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
